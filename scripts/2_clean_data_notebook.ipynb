{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is dedicated fir creating and testing the success of the different modules, different steps of the cleaning function of the data. After all of the different modules will be tested and done, it will than be integrated into one main cleaning function and will be copied to the `2_clean_data.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script reads a raw CSV file, applies a series of text cleaning steps,\n",
    "(optional: and saves the cleaned output to a new CSV)\n",
    "\n",
    "Included cleaning steps:\n",
    "✔ Expand contractions (e.g., \"I'm\" → \"I am\")\n",
    "✔ Lowercasing\n",
    "✔ Normalize whitespace\n",
    "✔ Normalize repeated punctuation (e.g., \"!!!\" → \"!\")\n",
    "✔ Drop empty or invalid rows\n",
    "\n",
    "\n",
    "Excluded steps:\n",
    "✘ Spelling correction – Excluded because:\n",
    "    - May over-correct expressive/emotional writing (e.g., \"soooo happy\")\n",
    "    - Adds significant runtime\n",
    "    - Low value in this domain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import contractions\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape is (16000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data:\n",
    "df = pd.read_csv(r\"C:\\Users\\Inbal\\projects\\emotion_detection\\data\\train.csv\")\n",
    "\n",
    "print(f\"df.shape is {df.shape}\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the modules need to be implemented until combine into main clean_df function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. fix_informal_contractions(text)\n",
    "Convert variants like im, i m, Im, Dont, don t → standardized forms (I'm, don't, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. fix_informal_contractions(text: str) -> str\n",
    "# Convert variants like im, i m, Im, Dont, don t (which exist in this dataset) → standardized forms (I'm, don't, etc.)\n",
    "\n",
    "def fix_informal_contractions(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize sloppy/missing-apostrophe contractions into their canonical apostrophized forms,\n",
    "    so that downstream expansion (e.g. with contractions.fix) works reliably.\n",
    "\n",
    "    Specifically handles patterns like:\n",
    "      - \"im\", \"i m\", \"Im\", \"I m\"       → \"I'm\"\n",
    "      - \"dont\", \"don t\", \"Dont\", \"Don t\" → \"don't\"\n",
    "      - (and similarly for: didnt, ive, wasnt, doesnt, shouldnt, cant, wont, couldnt, wouldnt)\n",
    "\n",
    "    We use word-boundary regex (\\b) to avoid partial matches, and include both lowercase\n",
    "    and capitalized variants so we catch every case.\n",
    "    \"\"\"\n",
    "    # Mapping of regex → replacement\n",
    "    pre_map = {\n",
    "        # “I’m” variants\n",
    "        r\"\\b(i\\s?m|im|Im|I\\s?m)\\b\": \"I'm\",\n",
    "        # “don't” variants\n",
    "        r\"\\b(dont|don\\s?t|Dont|Don\\s?t)\\b\": \"don't\",\n",
    "        # “didn't”\n",
    "        r\"\\b(didnt|did\\s?nt|Didnt|Did\\s?nt)\\b\": \"didn't\",\n",
    "        # “I've”\n",
    "        r\"\\b(ive|i\\s?ve|Ive|I\\s?ve)\\b\": \"I've\",\n",
    "        # “wasn't”\n",
    "        r\"\\b(wasnt|was\\s?nt|Wasnt|Was\\s?nt)\\b\": \"wasn't\",\n",
    "        # “doesn't”\n",
    "        r\"\\b(doesnt|does\\s?nt|Doesnt|Does\\s?nt)\\b\": \"doesn't\",\n",
    "        # “shouldn't”\n",
    "        r\"\\b(shouldnt|should\\s?nt|Shouldnt|Should\\s?nt)\\b\": \"shouldn't\",\n",
    "        # “can't”\n",
    "        r\"\\b(cant|can\\s?t|Cant|Can\\s?t)\\b\": \"can't\",\n",
    "        # “won't”\n",
    "        r\"\\b(wont|won\\s?t|Wont|Won\\s?t)\\b\": \"won't\",\n",
    "        # “couldn't”\n",
    "        r\"\\b(couldnt|could\\s?nt|Couldnt|Could\\s?nt)\\b\": \"couldn't\",\n",
    "        # “wouldn't”\n",
    "        r\"\\b(wouldnt|would\\s?nt|Wouldnt|Would\\s?nt)\\b\": \"wouldn't\",\n",
    "    }\n",
    "\n",
    "    # Apply each pattern in turn\n",
    "    for pattern, replacement in pre_map.items():\n",
    "        # We don’t set IGNORECASE because we explicitly list capitalized forms\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = \"im grabbing a minute to post i feel greedy wrong\"\n",
    "# b = \"Im grabbing a minute to post i feel greedy wrong\"\n",
    "# c = \"i m grabbing a minute to post i feel greedy wrong\"\n",
    "# d = \"I m grabbing a minute to post i feel greedy wrong\"\n",
    "# e = \"i\\m grabbing a minute to post i feel greedy wrong\"\n",
    "# f = \"I\\m grabbing a minute to post i feel greedy wrong\"\n",
    "\n",
    "# g = \"i?m grabbing a minute to post i feel greedy wrong\"\n",
    "# h = \"I?m grabbing a minute to post i feel greedy wrong\"\n",
    "\n",
    "# i = \"i   m grabbing a minute to post i feel greedy wrong\"\n",
    "# j = \"I   m grabbing a minute to post i feel greedy wrong\"\n",
    "\n",
    "\n",
    "# for z in [a, b, c, d, e, f, g, h, i, j]:\n",
    "#     print(z)\n",
    "#     r = fix_informal_contractions(z)\n",
    "#     print(r)\n",
    "#     print('\\n\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. expand_contractions(text: str) -> str\n",
    "Use contractions.fix() to turn I'm → I am, don't → do not, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. lowercase_text(text: str) -> str\n",
    "Normalize all characters to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. normalize_whitespace(text: str) -> str\n",
    "Collapse runs of whitespace (\\s+) to a single space and trim leading/trailing spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. normalize_punctuation(text: str) -> str\n",
    "Replace repeated punctuation sequences (!!!, ..., ???) with a single character (!, ., ?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. clean_text(text: str) -> str\n",
    "(Optional helper) Wraps steps 1–5 in one function for single-string cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7. drop_invalid_rows(df: pd.DataFrame, col: str = \"text\") -> pd.DataFrame\n",
    "Remove any rows where df[col] is null or empty after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finaly:\n",
    "clean_df(df: pd.DataFrame, col: str = \"text\") -> pd.DataFrame\n",
    "Master pipeline that:\n",
    "\n",
    "* Ensures col exists and is string\n",
    "\n",
    "* Applies clean_text (or steps 1–5) to df[col]\n",
    "\n",
    "* Calls drop_invalid_rows\n",
    "\n",
    "* Returns the cleaned DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output comparison between raw and clean DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_examples(df, emotion, n_samples):\n",
    "#     samples = df[df.emotion == emotion].sample(n_samples, random_state=42)\n",
    "#     for idx, row in samples.iterrows():\n",
    "#         print(f\"# {row.text}\")\n",
    "#     print(\"\\n\")\n",
    "\n",
    "\n",
    "# def output_comparison(raw_df, clean_df):\n",
    "#     emotions = sorted(raw_df.emotion.unique())\n",
    "\n",
    "#     for emotion in emotions:\n",
    "#         print(f\"--- {emotion.upper()} ---\")\n",
    "\n",
    "#         print(\"Raw Examples:\")\n",
    "#         print_examples(raw_df, emotion, 3)\n",
    "\n",
    "#         print(\"Clean Examples:\")\n",
    "#         print_examples(clean_df, emotion, 3)\n",
    "        \n",
    "#         print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_emotion_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
