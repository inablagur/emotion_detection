# ğŸ˜Š Emotion Detection Project

## ğŸ“– Overview  
Build and compare multiple text-based emotion classifiers
- ğŸ—ï¸ **Shallow baseline**: (e.g. Logistic Regression, SVM, NaÃ¯ve Bayes over TFâ€“IDF features)
- ğŸ¤– **Transformer fine-tuning**: BERT-base-uncased & DistilBERT

Weâ€™ll evaluate each on accuracy, macro-F1, training time, and memory footprint, then draw final conclusions about their trade-offs.\

---

## ğŸ“¦ Dataset  
We use the **Kaggle â€œEmotionâ€** dataset (28 k samples, 6 labels - anger; fear; joy; love; sadness; surprise):  
- âœ”ï¸ Already labeled (CSV)  
- âš–ï¸ Balanced for clear comparisons  
- â±ï¸ Small enough to fine-tune in <30 min on GPU  

---

## ğŸ—‚ï¸ Project Structure
```text
emotion_detection/        
â”‚
â”œâ”€â”€ data/                             â† All raw & split datasets
â”‚   â”œâ”€â”€ train.csv                     â† Training examples (text + emotion)                    (Output of `scripts/1_load_data.py`)
â”‚   â”œâ”€â”€ validation.csv                â† Validation examples                                   (Output of `scripts/1_load_data.py`)
â”‚   â””â”€â”€ test.csv                      â† Test examples                                         (Output of `scripts/1_load_data.py`)
â”‚   â”œâ”€â”€ train_clean.csv               â† Preprocessed Training examples (text + emotion)       (Output of `scripts/2_clean_data.py`)
â”‚   â”œâ”€â”€ validation_clean.csv          â† Preprocessed Validation examples                      (Output of `scripts/2_clean_data.py`)
â”‚   â””â”€â”€ test_clean.csv                â† Preprocessed Test examples                            (Output of `scripts/2_clean_data.py`)
â”‚
â”œâ”€â”€ scripts/                          â† Helper scripts for data prep & model runs, executable entry points.
â”‚   â”œâ”€â”€ 1_load_data.py                â† Loads HF â€œemotionâ€ dataset, writes train/val/test CSVs
â”‚   â””â”€â”€ 2_clean_data.py               â† Preprocesses text for both shallow and transformer models                                        
â”‚
â”œâ”€â”€ notebooks/                        â† Jupyter notebooks, aligned with milestones
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb     â† Explore data: counts, lengths, quirks
â”‚   â”œâ”€â”€ 02_tfidf_baseline.ipynb       â† Baseline models with TFâ€“IDF + classifiers
â”‚   â”œâ”€â”€ 03_transformer_finetune.ipynb â† Fine-tune BERT & DistilBERT
â”‚   â””â”€â”€ 04_compare_and_plot.ipynb     â† Compare metrics, times, and produce visuals
â”‚
â”œâ”€â”€ models/                           â† Saved model checkpoints (populates later)
â”‚
â”œâ”€â”€ README.md                         â† High-level overview, setup, and workflow
â”œâ”€â”€ requirements.txt                  â† Pinned dependencies (Python 3.10)
â””â”€â”€ .gitignore                        â† Files/folders to omit from Git

```
---

## ğŸ”„ General Pipeline
(both) **Preprocessing**: lowercase, strip punctuation, expand contractions, pruning.

### 1ï¸âƒ£ Shallow Baseline  
1. **Vectorization**: TFâ€“IDF (`TfidfVectorizer(ngram_range=(1,2), max_features=10k)`)  
2. **Classification**: Logistic Regression (or SVM / NaÃ¯ve Bayes)  


### 2ï¸âƒ£ Transformer Fine-Tuning  
1. **Tokenization**: `AutoTokenizer` (`bert-base-uncased`)  
2. **Model**: `AutoModelForSequenceClassification` (+ 6-label head)  
3. **Training**: Hugging Face `Trainer` API  

(both) **Evaluation**: train/dev/test split â†’ accuracy, macro-F1, per-class F1

---

## ğŸ“Š Comparison & Visualization  
- **Metrics Table**: training time, test accuracy, macro-F1 for each model  
- **Bar Charts**: side-by-side accuracy and F1 comparisons  
- **Confusion Matrices**: per-class error patterns  
- **Optional Explainability**:  
  - SHAP / LIME for classical & deep models  
  - Attention-map visuals with BertViz or Captum  

---

## âš™ï¸ Setup

1. **Python & environment**  
   - Requires Python 3.8â€“3.10 (tested on 3.10)  
     
2. **Clone the repo**  
   ```bash
   git clone git@github.com:<YOUR_USERNAME>/emotion_detection.git
   cd emotion_detection

3. **Install Requirements**  
   ```bash
   pip install -r requirements.txt

4. **Prepare the data**  
   ```bash
   # Read and save the data
   python scripts/1_load_data.py

   # Preprocess and save the data
   python scripts/2_clean_data.py

5. **(To be added)**
    - Model training scripts (TF-IDF, transformer)
    - Evaluation & comparison tools
    - Demo deployment commands
---

## ğŸ“ Roadmap & Status

Below is a high-level checklist of our project milestones.  
âœ”ï¸ Completedâ€ƒâšªï¸ Pending

- âœ”ï¸ **Project Kick-off**  
  - [x] Repo created  
  - [x] README & .gitignore added  
  - [x] Environment & deps installed

- âšªï¸ **Data Gathering & Exploration**  
  - [x] Download & split dataset  
  - [x] `01_data_exploration.ipynb` completed

- âšªï¸ **Data Cleaning & Preprocessing**  
  - [x] Implement the modules individually and test them one by one
  - [x] Integrate all individual modules into main clean_df function and test
  - [x] Integrate into `scripts/2_clean_data.py`

- âšªï¸ **Build Simple (Shallow) Models**  
  - [ ] `02_tfidf_baseline.ipynb`  
  - [ ] Evaluate & record metrics

- âšªï¸ **Build Transformer Models**  
  - [ ] `03_transformer_finetune.ipynb`  
  - [ ] Track train time / memory

- âšªï¸ **Compare & Visualize Results**  
  - [ ] `04_compare_and_plot.ipynb`

- âšªï¸ **(Optional) Explainability**  
  - [ ] `05_explainability.ipynb`

- âšªï¸ **Polish & Publish**  
  - [ ] Ensure persisnence between code and files
  - [ ] Final README updates  
  - [ ] Demo deployment / release tag