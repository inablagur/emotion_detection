# ğŸ˜Š Emotion Detection Project

## ğŸ“– Overview  
Build and compare multiple text-based emotion classifiers
- ğŸ—ï¸ **Shallow baseline**: (e.g. Logistic Regression, SVM, NaÃ¯ve Bayes over TFâ€“IDF features)
- ğŸ¤– **Transformer fine-tuning**: BERT-base-uncased & DistilBERT

Weâ€™ll evaluate each on accuracy, macro-F1, training time, and memory footprint, then draw final conclusions about their trade-offs.\

---

## ğŸ“¦ Dataset  
We use the **Kaggle â€œEmotionâ€** dataset (28 k samples, 6 labels - anger; fear; joy; love; sadness; surprise):  
- âœ”ï¸ Already labeled (CSV)  
- âš–ï¸ Balanced for clear comparisons  
- â±ï¸ Small enough to fine-tune in <30 min on GPU  

---

## ğŸ—‚ï¸ Project Structure
```text
emotion_detection_project/
â”œâ”€â”€ data/                # raw & processed data
â”œâ”€â”€ notebooks/           # exploration & training notebooks
â”œâ”€â”€ scripts/             # batch/CI scripts
â”œâ”€â”€ models/              # saved checkpoints
â”œâ”€â”€ README.md            # this file
â”œâ”€â”€ requirements.txt     # Python deps
â””â”€â”€ .gitignore           # ignore rules
```
---

## ğŸ”„ General Pipeline

### 1ï¸âƒ£ Shallow Baseline  
1. **Preprocessing**: lowercase, strip punctuation, optional stop-words  
2. **Vectorization**: TFâ€“IDF (`TfidfVectorizer(ngram_range=(1,2), max_features=10k)`)  
3. **Classification**: Logistic Regression (or SVM / NaÃ¯ve Bayes)  
4. **Evaluation**: train/dev/test split â†’ accuracy, macro-F1, per-class F1

### 2ï¸âƒ£ Transformer Fine-Tuning  
1. **Tokenization**: `AutoTokenizer` (`bert-base-uncased`)  
2. **Model**: `AutoModelForSequenceClassification` (+ 6-label head)  
3. **Training**: Hugging Face `Trainer` API  
4. **Evaluation**: same splits & metrics as baseline

---

## ğŸ“Š Comparison & Visualization  
- **Metrics Table**: training time, test accuracy, macro-F1 for each model  
- **Bar Charts**: side-by-side accuracy and F1 comparisons  
- **Confusion Matrices**: per-class error patterns  
- **Optional Explainability**:  
  - SHAP / LIME for classical & deep models  
  - Attention-map visuals with BertViz or Captum  

---

## âš™ï¸ Setup

1. **Clone the repo**  
   ```bash
   git clone git@github.com:YOUR_USERNAME/emotion_detection_project.git
   cd emotion_detection_project

2. **Install Requirements**  
   ```bash
   pip install -r requirements.txt

3. **Kick off**  
   ```bash
   python scripts/tfidf_baseline.py --data_path data/emotion.csv --output_dir models/tfidf
